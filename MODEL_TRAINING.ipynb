{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60276f13-b178-4fe8-b5a6-a3dacefb2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Multi-Task RoBERTa Model Training\n",
    "# ============================================\n",
    "\n",
    "# Install transformers and training libraries\n",
    "!pip install transformers datasets torch scikit-learn accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe3cf42-617a-4285-b7b5-b03856937e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    RobertaTokenizer, \n",
    "    RobertaModel, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f16ac-e8de-45e5-8d31-bc7c37dcbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "  from .autonotebook import tqdm as notebook_tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f2ada2-789b-4039-810b-8d33e5df4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW # New/Standard way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68c0171-001a-4701-b919-248be19d3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301b1728-52b1-4595-8f20-06ec569949e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING PROCESSED DATA\n",
      "============================================================\n",
      "‚úÖ Loaded 10000 sentiment samples\n",
      "‚úÖ Loaded 138 intent samples\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD PROCESSED DATA\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sentiment_df = pd.read_csv('data/processed/sentiment_processed.csv')\n",
    "intent_df = pd.read_csv('data/processed/intent_processed.csv')\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(sentiment_df)} sentiment samples\")\n",
    "print(f\"‚úÖ Loaded {len(intent_df)} intent samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774861f0-afaf-454d-92f7-193305a996b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENCODING LABELS\n",
      "============================================================\n",
      "Sentiment classes: ['negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# PREPARE LABELS\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Encode sentiment labels\n",
    "sentiment_encoder = LabelEncoder()\n",
    "sentiment_df['sentiment_encoded'] = sentiment_encoder.fit_transform(sentiment_df['sentiment'])\n",
    "sentiment_classes = sentiment_encoder.classes_\n",
    "print(f\"Sentiment classes: {sentiment_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37493caf-d573-4f74-8572-bb2057d63ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent classes (22): ['Clever' 'CourtesyGoodBye' 'CourtesyGreeting' 'CourtesyGreetingResponse'\n",
      " 'CurrentHumanQuery' 'GoodBye' 'Gossip' 'Greeting' 'GreetingResponse'\n",
      " 'Jokes']...\n"
     ]
    }
   ],
   "source": [
    "# Encode intent labels\n",
    "intent_encoder = LabelEncoder()\n",
    "intent_df['intent_encoded'] = intent_encoder.fit_transform(intent_df['intent'])\n",
    "intent_classes = intent_encoder.classes_\n",
    "print(f\"Intent classes ({len(intent_classes)}): {intent_classes[:10]}...\")  # Show first 10\n",
    "\n",
    "# Save encoders\n",
    "encoders = {\n",
    "    'sentiment_classes': sentiment_classes.tolist(),\n",
    "    'intent_classes': intent_classes.tolist()\n",
    "}\n",
    "with open('models/label_encoders.json', 'w') as f:\n",
    "    json.dump(encoders, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6306738f-f076-44ef-ae83-6212303893a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SPLITTING DATA\n",
      "============================================================\n",
      "Sentiment - Train: 700, Val: 150, Test: 150\n",
      "‚úÖ Intent Split Success!\n",
      "Train: 96, Val: 21, Test: 21\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRAIN/VAL/TEST SPLIT\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPLITTING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CRITICAL: Shrink sentiment data to 1000 rows so CPU training finishes today\n",
    "sentiment_df = sentiment_df.sample(n=1000, random_state=42)\n",
    "# Split sentiment data (70% train, 15% val, 15% test)\n",
    "sent_train, sent_temp = train_test_split(sentiment_df, test_size=0.3, random_state=42, stratify=sentiment_df['sentiment_encoded'])\n",
    "sent_val, sent_test = train_test_split(sent_temp, test_size=0.5, random_state=42, stratify=sent_temp['sentiment_encoded'])\n",
    "\n",
    "print(f\"Sentiment - Train: {len(sent_train)}, Val: {len(sent_val)}, Test: {len(sent_test)}\")\n",
    "\n",
    "# Split intent data\n",
    "intent_train, intent_temp = train_test_split(\n",
    "    intent_df, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=intent_df['intent_encoded']\n",
    ")\n",
    "\n",
    "# Second split: Split temp (30%) into Val (15%) and Test (15%)\n",
    "# REMOVE stratify here to avoid the ValueError\n",
    "intent_val, intent_test = train_test_split(\n",
    "    intent_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"‚úÖ Intent Split Success!\")\n",
    "print(f\"Train: {len(intent_train)}, Val: {len(intent_val)}, Test: {len(intent_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0370601c-679d-4716-b25d-257aadc3c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CUSTOM DATASET CLASS\n",
    "# ===========================\n",
    "\n",
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-task learning\"\"\"\n",
    "    \n",
    "    def __init__(self, sentiment_data, intent_data, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Combine both datasets\n",
    "        self.samples = []\n",
    "        \n",
    "        # Add sentiment samples\n",
    "        for _, row in sentiment_data.iterrows():\n",
    "            self.samples.append({\n",
    "                'text': row['text_cleaned'],\n",
    "                'sentiment_label': row['sentiment_encoded'],\n",
    "                'intent_label': -1,  # No intent label\n",
    "                'task': 'sentiment'\n",
    "            })\n",
    "        \n",
    "        # Add intent samples\n",
    "        for _, row in intent_data.iterrows():\n",
    "            self.samples.append({\n",
    "                'text': row['text_cleaned'],\n",
    "                'sentiment_label': -1,  # No sentiment label\n",
    "                'intent_label': row['intent_encoded'],\n",
    "                'task': 'intent'\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            sample['text'],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'sentiment_label': torch.tensor(sample['sentiment_label'], dtype=torch.long),\n",
    "            'intent_label': torch.tensor(sample['intent_label'], dtype=torch.long),\n",
    "            'task': sample['task']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "472ecbf4-b38b-4c62-affd-96d6ebf74691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# MULTI-TASK MODEL ARCHITECTURE\n",
    "# ===========================\n",
    "\n",
    "class MultiTaskRoBERTa(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task RoBERTa model with two classification heads:\n",
    "    - Sentiment classification (2 classes: positive/negative)\n",
    "    - Intent classification (N classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_sentiment_classes, num_intent_classes, dropout=0.3):\n",
    "        super(MultiTaskRoBERTa, self).__init__()\n",
    "        \n",
    "        # Shared RoBERTa base\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Sentiment classification head\n",
    "        self.sentiment_classifier = nn.Linear(self.roberta.config.hidden_size, num_sentiment_classes)\n",
    "        \n",
    "        # Intent classification head\n",
    "        self.intent_classifier = nn.Linear(self.roberta.config.hidden_size, num_intent_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get RoBERTa outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Get logits from both heads\n",
    "        sentiment_logits = self.sentiment_classifier(pooled_output)\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        \n",
    "        return sentiment_logits, intent_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20a6f21-3897-4f9b-8d2e-04b4feb1126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e3f273-f92e-4b9c-9d60-1fdd9455d44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING MODEL\n",
      "============================================================\n",
      "‚è≥ Attempting to download/load RoBERTa (this may take a few mins)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Connection still failing: RobertaModel.__init__() got an unexpected keyword argument 'resume_download'\n",
      "üîÑ Checking if a local copy exists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà| 197/197 [00:01<00:00, 159.17it/s, Materializing param=encoder.layer.11.output.dense.weight]\n",
      "RobertaModel LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found local cache!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# INITIALIZE MODEL & TOKENIZER\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "\n",
    "# Set a much longer timeout for slow connections\n",
    "os.environ['HTTpx_TIMEOUT'] = '600' \n",
    "\n",
    "print(\"‚è≥ Attempting to download/load RoBERTa (this may take a few mins)...\")\n",
    "\n",
    "try:\n",
    "    # 1. Try loading normally with a forced download if needed\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name, resume_download=True)\n",
    "    base_model = RobertaModel.from_pretrained(model_name, resume_download=True)\n",
    "    print(\"‚úÖ Successfully downloaded from Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Connection still failing: {e}\")\n",
    "    print(\"üîÑ Checking if a local copy exists...\")\n",
    "    # 2. Fallback: try loading only from local cache\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    base_model = RobertaModel.from_pretrained(model_name, local_files_only=True)\n",
    "    print(\"‚úÖ Found local cache!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88f4037-faff-4e47-90aa-e4865293351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING DATALOADERS\n",
      "============================================================\n",
      "‚úÖ Train batches: 50\n",
      "‚úÖ Val batches: 11\n",
      "‚úÖ Test batches: 11\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CREATE DATALOADERS\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING DATALOADERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_dataset = MultiTaskDataset(sent_train, intent_train, tokenizer)\n",
    "val_dataset = MultiTaskDataset(sent_val, intent_val, tokenizer)\n",
    "test_dataset = MultiTaskDataset(sent_test, intent_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(f\"‚úÖ Train batches: {len(train_loader)}\")\n",
    "print(f\"‚úÖ Val batches: {len(val_loader)}\")\n",
    "print(f\"‚úÖ Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36e264f-d37a-4f51-9be0-2737516191e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 3: INITIALIZING TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà| 197/197 [00:01<00:00, 167.85it/s, Materializing param=encoder.layer.11.output.dense.weight]\n",
      "RobertaModel LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3\n",
      "Learning rate: 2e-05\n",
      "Total training steps: 150\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRAINING SETUP\n",
    "# ===========================\n",
    "# ============================================================\n",
    "# TRAINING SETUP & CONFIGURATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: INITIALIZING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Configuration\n",
    "EPOCHS = 3 \n",
    "LEARNING_RATE = 2e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Build the Multi-Task Model\n",
    "# We use 'base_model' which was the RoBERTa we loaded from cache earlier\n",
    "model = MultiTaskRoBERTa(\n",
    "    num_sentiment_classes=len(sentiment_classes),\n",
    "    num_intent_classes=len(intent_classes)\n",
    ")\n",
    "model.roberta = base_model  # Inject the pre-trained weights\n",
    "model.to(device)\n",
    "\n",
    "# 3. Loss Functions\n",
    "sentiment_criterion = nn.CrossEntropyLoss()\n",
    "intent_criterion = nn.CrossEntropyLoss(ignore_index=-1) # Skip the 'empty' labels\n",
    "\n",
    "# 4. Optimizer (Imported from torch.optim to avoid library version errors)\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 5. Scheduler (Crucial for Transformers)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f560198-4674-4a4b-ba0f-5793ec0db50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# CORRECTED TRAINING & EVAL FUNCTIONS\n",
    "# ===========================\n",
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss, s_correct, s_total, i_correct, i_total = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        s_labels, i_labels = batch['sentiment_label'].to(device), batch['intent_label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        s_logits, i_logits = model(input_ids, mask)\n",
    "        \n",
    "        # MASKING: Only calculate loss for valid labels (!= -1)\n",
    "        sent_mask, int_mask = s_labels != -1, i_labels != -1\n",
    "        loss = 0\n",
    "        if sent_mask.any():\n",
    "            loss += sentiment_criterion(s_logits[sent_mask], s_labels[sent_mask])\n",
    "            s_correct += (torch.argmax(s_logits[sent_mask], dim=1) == s_labels[sent_mask]).sum().item()\n",
    "            s_total += sent_mask.sum().item()\n",
    "        if int_mask.any():\n",
    "            loss += intent_criterion(i_logits[int_mask], i_labels[int_mask])\n",
    "            i_correct += (torch.argmax(i_logits[int_mask], dim=1) == i_labels[int_mask]).sum().item()\n",
    "            i_total += int_mask.sum().item()\n",
    "        \n",
    "        if isinstance(loss, torch.Tensor):\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss/len(dataloader), s_correct/s_total, i_correct/i_total\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, s_correct, s_total, i_correct, i_total = 0, 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            s_labels, i_labels = batch['sentiment_label'].to(device), batch['intent_label'].to(device)\n",
    "            s_logits, i_logits = model(input_ids, mask)\n",
    "            \n",
    "            sent_mask, int_mask = s_labels != -1, i_labels != -1\n",
    "            loss = 0\n",
    "            if sent_mask.any():\n",
    "                loss += sentiment_criterion(s_logits[sent_mask], s_labels[sent_mask])\n",
    "                s_correct += (torch.argmax(s_logits[sent_mask], dim=1) == s_labels[sent_mask]).sum().item()\n",
    "                s_total += sent_mask.sum().item()\n",
    "            if int_mask.any():\n",
    "                loss += intent_criterion(i_logits[int_mask], i_labels[int_mask])\n",
    "                i_correct += (torch.argmax(i_logits[int_mask], dim=1) == i_labels[int_mask]).sum().item()\n",
    "                i_total += int_mask.sum().item()\n",
    "            total_loss += loss.item() if isinstance(loss, torch.Tensor) else 0\n",
    "            \n",
    "    return total_loss/len(dataloader), s_correct/(s_total or 1), i_correct/(i_total or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eb53b75-d3d8-4ff4-b2c8-6350f1b08906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_sentiment_acc': [],\n",
    "    'val_sentiment_acc': [],\n",
    "    'train_intent_acc': [],\n",
    "    'val_intent_acc': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3d8347-a576-47d3-ba7a-6a1cddd2caa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [],\n",
       " 'val_loss': [],\n",
       " 'train_sentiment_acc': [],\n",
       " 'val_sentiment_acc': [],\n",
       " 'train_intent_acc': [],\n",
       " 'val_intent_acc': []}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f3c2ae3-e2ae-4ef4-9895-c4913ab2d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration re-loaded. Ready to train for 3 epochs.\n"
     ]
    }
   ],
   "source": [
    "# Re-declaring variables to ensure they are in memory\n",
    "EPOCHS = 3 \n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "# Ensure the history dictionary exists\n",
    "if 'history' not in locals():\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_sentiment_acc': [], 'val_sentiment_acc': [],\n",
    "        'train_intent_acc': [], 'val_intent_acc': []\n",
    "    }\n",
    "\n",
    "print(f\"‚úÖ Configuration re-loaded. Ready to train for {EPOCHS} epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d00a8e7c-c370-4d08-a6a1-c5ae2aaacc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [13:38<00:00, 16.37s/it]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:45<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1983 | Val Loss: 1.0917\n",
      "Train Sentiment Acc: 0.5514 | Val Sentiment Acc: 0.6600\n",
      "Train Intent Acc: 0.0833 | Val Intent Acc: 0.1429\n",
      "\n",
      "üìä Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [12:21<00:00, 14.82s/it]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:37<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1687 | Val Loss: 0.8566\n",
      "Train Sentiment Acc: 0.8300 | Val Sentiment Acc: 0.8333\n",
      "Train Intent Acc: 0.1146 | Val Intent Acc: 0.2857\n",
      "\n",
      "üìä Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [11:32<00:00, 13.84s/it]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:41<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5846 | Val Loss: 0.7639\n",
      "Train Sentiment Acc: 0.9071 | Val Sentiment Acc: 0.8400\n",
      "Train Intent Acc: 0.3750 | Val Intent Acc: 0.5238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nüìä Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_sent_acc, train_intent_acc = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    val_loss, val_sent_acc, val_intent_acc = evaluate(model, val_loader)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_sentiment_acc'].append(train_sent_acc)\n",
    "    history['val_sentiment_acc'].append(val_sent_acc)\n",
    "    history['train_intent_acc'].append(train_intent_acc)\n",
    "    history['val_intent_acc'].append(val_intent_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Sentiment Acc: {train_sent_acc:.4f} | Val Sentiment Acc: {val_sent_acc:.4f}\")\n",
    "    print(f\"Train Intent Acc: {train_intent_acc:.4f} | Val Intent Acc: {val_intent_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "557ae16a-2345-40c9-a14c-c69dd998b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:33<00:00,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Test Results:\n",
      "Loss: 0.7777\n",
      "Sentiment Accuracy: 0.8467\n",
      "Intent Accuracy: 0.3810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#===========================\n",
    "# FINAL EVALUATION ON TEST SET\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_loss, test_sent_acc, test_intent_acc = evaluate(model, test_loader)\n",
    "print(f\"\\nüéØ Test Results:\")\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"Sentiment Accuracy: {test_sent_acc:.4f}\")\n",
    "print(f\"Intent Accuracy: {test_intent_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aff2231f-e881-47e3-8a2a-a7e0aa82a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODEL\n",
      "============================================================\n",
      "‚úÖ Successfully saved to models/multitask_roberta\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# SAVE MODEL (FINAL VERSION)\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# This path includes the subfolder\n",
    "model_save_path = 'models/multitask_roberta'\n",
    "\n",
    "# This ensures the specific subfolder exists\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# 1. Save weights and metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'sentiment_classes': sentiment_classes.tolist(),\n",
    "    'intent_classes': intent_classes.tolist(),\n",
    "    'history': history\n",
    "}, f'{model_save_path}/model.pth')\n",
    "\n",
    "# 2. Save tokenizer (crucial for Streamlit)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# 3. Save classes as JSON (makes Streamlit loading much faster)\n",
    "classes_data = {\n",
    "    'sentiment_classes': sentiment_classes.tolist(),\n",
    "    'intent_classes': intent_classes.tolist()\n",
    "}\n",
    "with open(f'{model_save_path}/classes.json', 'w') as f:\n",
    "    json.dump(classes_data, f)\n",
    "\n",
    "print(f\"‚úÖ Successfully saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
