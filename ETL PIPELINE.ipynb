{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f76c1c-40ec-414a-8fb0-9d51ee4faf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.2.6)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (1.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from nltk) (1.5.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.9 MB/s  0:00:00\n",
      "Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/3 [regex]\n",
      "   ---------------------------------------- 0/3 [regex]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   -------------------------- ------------- 2/3 [nltk]\n",
      "   ---------------------------------------- 3/3 [nltk]\n",
      "\n",
      "Successfully installed click-8.3.1 nltk-3.9.2 regex-2026.1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Hp\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "C:\\Users\\Hp\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    " #Stage 2: ETL Pipeline with NLP Preprocessing\n",
    "# ==============================================\n",
    "\n",
    "# Install NLP libraries\n",
    "!pip install pandas numpy nltk  scikit-learn \n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sqlite3\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6c7050f-ee90-4964-8820-5a69d2b58fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.11-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from spacy) (25.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-extensions>=4.14.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp313-cp313-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.0/14.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.6/14.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.4/14.2 MB 3.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.9/14.2 MB 3.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.7/14.2 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 4.2/14.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.7/14.2 MB 2.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 5.5/14.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.0/14.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.8/14.2 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.4/14.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.9/14.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/14.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.2/14.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/14.2 MB 2.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.5/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.1/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 2.9 MB/s  0:00:04\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.9 MB/s  0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/653.1 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 524.3/653.1 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 653.1/653.1 kB 2.8 MB/s  0:00:00\n",
      "Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.8 MB/s  0:00:00\n",
      "Downloading blis-1.3.3-cp313-cp313-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.2 MB 3.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.0/6.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.2 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.1/6.2 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.8/6.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 3.0 MB/s  0:00:02\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, wasabi, typing-extensions, spacy-loggers, spacy-legacy, murmurhash, cymem, cloudpathlib, catalogue, blis, annotated-types, typing-inspection, typer-slim, srsly, smart-open, pydantic-core, preshed, pydantic, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/22 [wrapt]\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "  Attempting uninstall: typing-extensions\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "   - --------------------------------------  1/22 [wasabi]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   --- ------------------------------------  2/22 [typing-extensions]\n",
      "   ----- ----------------------------------  3/22 [spacy-loggers]\n",
      "   ----- ----------------------------------  3/22 [spacy-loggers]\n",
      "   ------- --------------------------------  4/22 [spacy-legacy]\n",
      "   ------- --------------------------------  4/22 [spacy-legacy]\n",
      "   --------- ------------------------------  5/22 [murmurhash]\n",
      "   ------------ ---------------------------  7/22 [cloudpathlib]\n",
      "   ------------ ---------------------------  7/22 [cloudpathlib]\n",
      "   ------------ ---------------------------  7/22 [cloudpathlib]\n",
      "   ------------ ---------------------------  7/22 [cloudpathlib]\n",
      "   ------------ ---------------------------  7/22 [cloudpathlib]\n",
      "   ---------------- -----------------------  9/22 [blis]\n",
      "   ---------------- -----------------------  9/22 [blis]\n",
      "   ------------------ --------------------- 10/22 [annotated-types]\n",
      "   --------------------- ------------------ 12/22 [typer-slim]\n",
      "   --------------------- ------------------ 12/22 [typer-slim]\n",
      "   --------------------- ------------------ 12/22 [typer-slim]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ----------------------- ---------------- 13/22 [srsly]\n",
      "   ------------------------- -------------- 14/22 [smart-open]\n",
      "   ------------------------- -------------- 14/22 [smart-open]\n",
      "   ------------------------- -------------- 14/22 [smart-open]\n",
      "   --------------------------- ------------ 15/22 [pydantic-core]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   ------------------------------ --------- 17/22 [pydantic]\n",
      "   -------------------------------- ------- 18/22 [confection]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ---------------------------------- ----- 19/22 [weasel]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   ------------------------------------ --- 20/22 [thinc]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   -------------------------------------- - 21/22 [spacy]\n",
      "   ---------------------------------------- 22/22 [spacy]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 typing-extensions-4.15.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Hp\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.0/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 2.6/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.1/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.0/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.6 MB/s  0:00:04\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Hp\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d18cce-34b0-4b68-9500-d746a1c8856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe68c38-b1df-46e4-8fde-01eb69b55ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34177fe5-ff3a-416f-b27e-23e721cad998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22f0540-e37d-4200-a935-0177590039b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loaded paths configuration\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD PATHS\n",
    "# ===========================\n",
    "with open('data/paths_config.json', 'r') as f:\n",
    "    paths = json.load(f)\n",
    "\n",
    "print(\"üìÇ Loaded paths configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9682485-8306-47d8-b10d-f344cb21d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 1: EXTRACT - Loading Raw Data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# EXTRACT: Load Raw Data\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 1: EXTRACT - Loading Raw Data\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b6b224a-cc61-4436-91f0-61c1d0c40328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentiment Data (Amazon Reviews)\n",
    "def load_fasttext_format(filepath):\n",
    "    \"\"\"Load FastText format: __label__X text\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('__label__'):\n",
    "                # Extract label and text\n",
    "                parts = line.split(' ', 1)\n",
    "                label = parts[0].replace('__label__', '')\n",
    "                text = parts[1] if len(parts) > 1 else ''\n",
    "                \n",
    "                labels.append(int(label))  # 1 or 2\n",
    "                texts.append(text)\n",
    "    \n",
    "    return pd.DataFrame({'text': texts, 'sentiment': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de0b765-114f-4f67-8880-eda7acc37b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7125015b-889c-4ad2-9d74-5dcab392d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading compressed Sentiment data...\n",
      "‚úÖ Loaded 143 intent samples from JSON\n"
     ]
    }
   ],
   "source": [
    "# Try loading sentiment data\n",
    "# 1. Load Sentiment Data (Handling the BZ2 format)\n",
    "sentiment_file_bz2 = os.path.join(paths['sentiment_path'], 'train.ft.txt.bz2')\n",
    "\n",
    "if os.path.exists(sentiment_file_bz2):\n",
    "    print(\"üì¶ Loading compressed Sentiment data...\")\n",
    "    # Use the logic we built: stream from bz2, split at first space\n",
    "    data = []\n",
    "    with bz2.open(sentiment_file_bz2, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 10000: break  # Sampling 10k\n",
    "            parts = line.split(\" \", 1)\n",
    "            if len(parts) > 1:\n",
    "                label = 1 if parts[0] == \"__label__2\" else 0 # 1 for positive\n",
    "                data.append({\"text\": parts[1].strip(), \"label\": label})\n",
    "    sentiment_df = pd.DataFrame(data)\n",
    "    sentiment_df['sentiment'] = sentiment_df['label'].map({0: 'negative', 1: 'positive'})\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è BZ2 file not found. Check paths.\")\n",
    "\n",
    "# 2. Load Intent Data (Fixing the ParserError)\n",
    "#2. Load Intent Data (Correctly as JSON)\n",
    "intent_file = os.path.join(paths['intent_path'], paths['intent_files'][0])\n",
    "\n",
    "try:\n",
    "    with open(intent_file, 'r', encoding='utf-8') as f:\n",
    "        raw_json = json.load(f)\n",
    "    \n",
    "    # Flatten the JSON (Intents -> Text patterns)\n",
    "    formatted_data = []\n",
    "    for item in raw_json['intents']:\n",
    "        tag = item['intent']\n",
    "        for pattern in item['text']:\n",
    "            formatted_data.append({'intent': tag, 'text': pattern})\n",
    "    \n",
    "    intent_df = pd.DataFrame(formatted_data)\n",
    "    print(f\"‚úÖ Loaded {len(intent_df)} intent samples from JSON\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Intent load failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa395ff-9b5b-4361-ac46-506376a993bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ NLTK Resources Downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This is the specific one the error asked for\n",
    "    nltk.download('punkt_tab')\n",
    "    # These are usually needed for your Preprocessor class\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    print(\"‚úÖ NLTK Resources Downloaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Download failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f53c4aa1-5573-4485-ac65-33f07f6d64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2: TRANSFORM - NLP Preprocessing\n",
      "============================================================\n",
      "üîÑ Processing sentiment data...\n",
      "üîÑ Processing intent data...\n",
      "Available columns: ['intent', 'text']\n",
      "‚úÖ Intent processing complete!\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# TRANSFORM: NLP Preprocessing\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2: TRANSFORM - NLP Preprocessing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Complete NLP preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic cleaning\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"Remove common stopwords\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered = [w for w in tokens if w not in self.stop_words]\n",
    "        return ' '.join(filtered)\n",
    "    \n",
    "    def lemmatize(self, text):\n",
    "        \"\"\"Lemmatize words to base form\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized = [self.lemmatizer.lemmatize(w) for w in tokens]\n",
    "        return ' '.join(lemmatized)\n",
    "    \n",
    "    def preprocess(self, text, remove_stops=True, lemmatize=True):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        # Clean\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if remove_stops:\n",
    "            text = self.remove_stopwords(text)\n",
    "        \n",
    "        # Lemmatize\n",
    "        if lemmatize:\n",
    "            text = self.lemmatize(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Preprocess sentiment data\n",
    "print(\"üîÑ Processing sentiment data...\")\n",
    "sentiment_df['text_original'] = sentiment_df['text']\n",
    "sentiment_df['text_cleaned'] = sentiment_df['text'].apply(preprocessor.preprocess)\n",
    "sentiment_df['text_length'] = sentiment_df['text_cleaned'].str.len()\n",
    "\n",
    "# Preprocess intent data (adjust column name as needed)\n",
    "print(\"üîÑ Processing intent data...\")\n",
    "# Let's see what we actually have\n",
    "print(f\"Available columns: {intent_df.columns.tolist()}\")\n",
    "\n",
    "# Ensure 'text' exists\n",
    "if 'text' in intent_df.columns:\n",
    "    intent_df['text_original'] = intent_df['text']\n",
    "    intent_df['text_cleaned'] = intent_df['text'].apply(preprocessor.preprocess)\n",
    "    intent_df['text_length'] = intent_df['text_cleaned'].str.len()\n",
    "else:\n",
    "    # If it's not named 'text', take the very first column available\n",
    "    first_col = intent_df.columns[0]\n",
    "    intent_df['text_original'] = intent_df[first_col]\n",
    "    intent_df['text_cleaned'] = intent_df[first_col].apply(preprocessor.preprocess)\n",
    "    \n",
    "# Safely handle the 'intent' column\n",
    "if 'intent' in intent_df.columns:\n",
    "    # It's already there, no need to do anything\n",
    "    pass\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: 'intent' column not found. Check your JSON flattening step.\")\n",
    "\n",
    "print(\"‚úÖ Intent processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0dd8af9-237e-4e4d-80fa-94596517a428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Quality Metrics:\n",
      "Sentiment - Empty texts: 0\n",
      "Intent - Empty texts: 5\n",
      "After cleaning - Sentiment: 10000, Intent: 138\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Data Quality Checks\n",
    "# ===========================\n",
    "print(\"\\nüìä Data Quality Metrics:\")\n",
    "print(f\"Sentiment - Empty texts: {(sentiment_df['text_cleaned'] == '').sum()}\")\n",
    "print(f\"Intent - Empty texts: {(intent_df['text_cleaned'] == '').sum()}\")\n",
    "\n",
    "# Remove empty texts\n",
    "sentiment_df = sentiment_df[sentiment_df['text_cleaned'].str.len() > 0]\n",
    "intent_df = intent_df[intent_df['text_cleaned'].str.len() > 0]\n",
    "\n",
    "print(f\"After cleaning - Sentiment: {len(sentiment_df)}, Intent: {len(intent_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6423f77-6001-451e-b3de-3b640558ea88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 3: LOAD - Storing in SQLite Warehouse\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LOAD: Store in SQLite\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 3: LOAD - Storing in SQLite Warehouse\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a285ebcb-8c33-4181-a2e1-e610328c98e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 10000 records in 'sentiment_data' table\n"
     ]
    }
   ],
   "source": [
    "# Create SQLite database\n",
    "db_path = 'data/support_warehouse.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Store sentiment data\n",
    "sentiment_df[['text_original', 'text_cleaned', 'sentiment', 'text_length']].to_sql(\n",
    "    'sentiment_data', \n",
    "    conn, \n",
    "    if_exists='replace', \n",
    "    index=False\n",
    ")\n",
    "print(f\"‚úÖ Stored {len(sentiment_df)} records in 'sentiment_data' table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8194ed7f-a22d-4dd9-aa15-09177b249f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stored 138 records in 'intent_data' table\n"
     ]
    }
   ],
   "source": [
    "# Store intent data\n",
    "intent_df[['text_original', 'text_cleaned', 'intent', 'text_length']].to_sql(\n",
    "    'intent_data',\n",
    "    conn,\n",
    "    if_exists='replace',\n",
    "    index=False\n",
    ")\n",
    "print(f\"‚úÖ Stored {len(intent_df)} records in 'intent_data' table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e18d5d7-267a-42d7-92d5-6fc7b7261ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Creating combined dataset for multi-task model...\n",
      "‚úÖ Stored 10138 records in 'combined_data' table\n"
     ]
    }
   ],
   "source": [
    "# Create combined table for multi-task learning\n",
    "print(\"\\nüîó Creating combined dataset for multi-task model...\")\n",
    "\n",
    "# Add source column\n",
    "sentiment_df['source'] = 'sentiment'\n",
    "intent_df['source'] = 'intent'\n",
    "\n",
    "# Combine (for multi-task, we'll use separate heads)\n",
    "combined_df = pd.DataFrame({\n",
    "    'text_original': list(sentiment_df['text_original']) + list(intent_df['text_original']),\n",
    "    'text_cleaned': list(sentiment_df['text_cleaned']) + list(intent_df['text_cleaned']),\n",
    "    'sentiment': list(sentiment_df['sentiment']) + [None] * len(intent_df),\n",
    "    'intent': [None] * len(sentiment_df) + list(intent_df['intent']),\n",
    "    'source': list(sentiment_df['source']) + list(intent_df['source'])\n",
    "})\n",
    "\n",
    "combined_df.to_sql('combined_data', conn, if_exists='replace', index=False)\n",
    "print(f\"‚úÖ Stored {len(combined_df)} records in 'combined_data' table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "419a271e-2df9-4979-89d4-57f7619b24c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ETL Pipeline Complete! Database saved at: data/support_warehouse.db\n"
     ]
    }
   ],
   "source": [
    "# Create metadata table\n",
    "metadata = {\n",
    "    'table_name': ['sentiment_data', 'intent_data', 'combined_data'],\n",
    "    'record_count': [len(sentiment_df), len(intent_df), len(combined_df)],\n",
    "    'created_date': [pd.Timestamp.now()] * 3\n",
    "}\n",
    "pd.DataFrame(metadata).to_sql('metadata', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()\n",
    "print(f\"\\n‚úÖ ETL Pipeline Complete! Database saved at: {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "245cc014-3aac-49c6-867d-519a79f455b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Next Step: Run 3_model_training.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Save processed data as CSV for model training\n",
    "sentiment_df.to_csv('data/processed/sentiment_processed.csv', index=False)\n",
    "intent_df.to_csv('data/processed/intent_processed.csv', index=False)\n",
    "combined_df.to_csv('data/processed/combined_processed.csv', index=False)\n",
    "\n",
    "print(\"\\nüìù Next Step: Run 3_model_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
